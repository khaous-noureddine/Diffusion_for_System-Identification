{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb105f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "###############################\n",
    "# 1) DATA LOADING & RESIZING\n",
    "###############################\n",
    "def load_swept_sine_case(file_path, amplitude_key, target_x=41, target_y=45):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        dataset = f['data_structure']['swept_sines'][amplitude_key]\n",
    "        wz_grid = np.array(dataset['wz_grid'])  # (6405, T)\n",
    "        T = wz_grid.shape[1]\n",
    "        spatial_x, spatial_y = 61, 105\n",
    "        wz_3d = wz_grid.reshape(spatial_x, spatial_y, T)\n",
    "        u = np.array(dataset['y'])\n",
    "        if u.ndim > 1:\n",
    "            u = u.squeeze()\n",
    "        frames_resized = np.zeros((target_x, target_y, T), dtype=np.float32)\n",
    "        for t in range(T):\n",
    "            frames_resized[:, :, t] = scipy.ndimage.zoom(\n",
    "                wz_3d[:, :, t],\n",
    "                (target_x / spatial_x, target_y / spatial_y),\n",
    "                order=1\n",
    "            )\n",
    "        frames_resized = np.transpose(frames_resized, (2, 0, 1))  # (T, target_x, target_y)\n",
    "        frames_resized = frames_resized[:, np.newaxis, :, :]      # (T, 1, target_x, target_y)\n",
    "        return {\n",
    "            'amplitude': amplitude_key,\n",
    "            'u': u.astype(np.float32),\n",
    "            'frames': frames_resized\n",
    "        }\n",
    "\n",
    "def load_all_amplitudes(file_path, amplitude_map, amplitude_list):\n",
    "    data_list = []\n",
    "    for amp in amplitude_list:\n",
    "        amp_key = amplitude_map[amp]\n",
    "        data_case = load_swept_sine_case(file_path, amp_key, target_x=41, target_y=45)\n",
    "        data_list.append(data_case)\n",
    "    return data_list\n",
    "\n",
    "def split_data(file_path):\n",
    "    amplitude_map = {\n",
    "        0.5:  'A0p05',\n",
    "        0.75: 'A0p075',\n",
    "        1.0:  'A0p10',\n",
    "        1.25: 'A0p125',\n",
    "        1.5:  'A0p15',\n",
    "        1.75: 'A0p175',\n",
    "        2.0:  'A0p20',\n",
    "        2.25: 'A0p225',\n",
    "        2.5:  'A0p25',\n",
    "        2.75: 'A0p275',\n",
    "        3.0:  'A0p30'\n",
    "    }\n",
    "    train_amps = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "    val_amps   = [0.75, 1.75, 2.75]\n",
    "    test_amps  = [1.25, 2.25]\n",
    "    train_list = load_all_amplitudes(file_path, amplitude_map, train_amps)\n",
    "    val_list   = load_all_amplitudes(file_path, amplitude_map, val_amps)\n",
    "    test_list  = load_all_amplitudes(file_path, amplitude_map, test_amps)\n",
    "    return train_list, val_list, test_list\n",
    "\n",
    "###############################\n",
    "# 1.1) NORMALISATION DES DONNÉES\n",
    "###############################\n",
    "def compute_normalization_stats(data_list):\n",
    "    all_frames = np.concatenate([data['frames'] for data in data_list], axis=0)  # (total_T, 1, H, W)\n",
    "    frame_mean = all_frames.mean()\n",
    "    frame_std = all_frames.std()\n",
    "    all_u = np.concatenate([data['u'] for data in data_list], axis=0)\n",
    "    u_mean = all_u.mean()\n",
    "    u_std = all_u.std()\n",
    "    return frame_mean, frame_std, u_mean, u_std\n",
    "\n",
    "def normalize_data_list(data_list, frame_mean, frame_std, u_mean, u_std):\n",
    "    for data in data_list:\n",
    "        data['frames'] = (data['frames'] - frame_mean) / frame_std\n",
    "        data['u'] = (data['u'] - u_mean) / u_std\n",
    "\n",
    "###############################\n",
    "# 2) CREATING SEQUENCES FOR ACDM\n",
    "###############################\n",
    "def create_acdm_sequences(data_list, past_window=2):\n",
    "    X_frames_list, X_u_past_list, X_u_curr_list, Y_list = [], [], [], []\n",
    "    for data_case in data_list:\n",
    "        frames = data_case['frames']  # (T, 1, H, W) - normalisées\n",
    "        u = data_case['u']            # (T,) - normalisé\n",
    "        T = frames.shape[0]\n",
    "        for i in range(past_window, T):\n",
    "            past_f = frames[i-past_window:i]          # (past_window, 1, H, W)\n",
    "            past_u = u[i-past_window:i].reshape(-1, 1)   # (past_window, 1)\n",
    "            current_u = np.array([u[i]], dtype=np.float32)  # (1,)\n",
    "            target_f = frames[i]                        # (1, H, W)\n",
    "            X_frames_list.append(past_f)\n",
    "            X_u_past_list.append(past_u)\n",
    "            X_u_curr_list.append(current_u)\n",
    "            Y_list.append(target_f)\n",
    "    X_frames = np.array(X_frames_list, dtype=np.float32)\n",
    "    X_u_past = np.array(X_u_past_list, dtype=np.float32)\n",
    "    X_u_curr = np.array(X_u_curr_list, dtype=np.float32)\n",
    "    Y = np.array(Y_list, dtype=np.float32)\n",
    "    return X_frames, X_u_past, X_u_curr, Y\n",
    "\n",
    "class ACylinderDataset(Dataset):\n",
    "    def init(self, X_frames, X_u_past, X_u_curr, Y):\n",
    "        self.X_frames = X_frames\n",
    "        self.X_u_past = X_u_past\n",
    "        self.X_u_curr = X_u_curr\n",
    "        self.Y = Y\n",
    "    def len(self):\n",
    "        return len(self.X_frames)\n",
    "    def getitem(self, idx):\n",
    "        return self.X_frames[idx], self.X_u_past[idx], self.X_u_curr[idx], self.Y[idx]\n",
    "\n",
    "###############################\n",
    "# 3) TIME EMBEDDING\n",
    "###############################\n",
    "def get_timestep_embedding(timesteps, embedding_dim, max_period=10000):\n",
    "    \"\"\"\n",
    "    Crée une embedding sinusoïdale pour les timesteps.\n",
    "    timesteps: tensor de forme (B,)\n",
    "    Retourne: tensor de forme (B, embedding_dim)\n",
    "    \"\"\"\n",
    "    half_dim = embedding_dim // 2\n",
    "    exponent = -math.log(max_period) * torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) / half_dim\n",
    "    emb = timesteps.float()[:, None] * exponent[None, :].exp()\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:  # pad si nécessaire\n",
    "        emb = F.pad(emb, (0, 1))\n",
    "    return emb\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def init(self, embedding_dim):\n",
    "        super(TimeEmbedding, self).init()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.linear1 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.linear2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "    def forward(self, t):\n",
    "        # t est un tensor de shape (B,)\n",
    "        emb = get_timestep_embedding(t, self.embedding_dim)\n",
    "        emb = self.linear1(emb)\n",
    "        emb = F.relu(emb)\n",
    "        emb = self.linear2(emb)\n",
    "        return emb\n",
    "\n",
    "###############################\n",
    "# 4) ATTENTION MODULES & RESIDUAL BLOCK\n",
    "###############################\n",
    "class SelfAttention(nn.Module):\n",
    "    def init(self, in_channels):\n",
    "        super(SelfAttention, self).init()\n",
    "        self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.key = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    def forward(self, x):\n",
    "        batch_size, C, height, width = x.size()\n",
    "        proj_query = self.query(x).view(batch_size, -1, height * width).permute(0, 2, 1)\n",
    "        proj_key = self.key(x).view(batch_size, -1, height * width)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy)\n",
    "        proj_value = self.value(x).view(batch_size, -1, height * width)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def init(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).init()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "###############################\n",
    "# 5) IMPROVED DEEP U-NET WITH TIME EMBEDDING, ATTENTION AND RESIDUAL CONNECTIONS\n",
    "###############################\n",
    "class ImprovedDeepUNet(nn.Module):\n",
    "    def init(self, in_channels, out_channels, features=[32, 64, 128, 256], time_emb_dim=256):\n",
    "        super(ImprovedDeepUNet, self).init()\n",
    "        self.time_emb_dim = time_emb_dim\n",
    "        # MLPs pour injecter le time embedding\n",
    "        self.time_mlp1 = nn.Linear(time_emb_dim, features[0])\n",
    "        self.time_mlp2 = nn.Linear(time_emb_dim, features[1])\n",
    "        self.time_mlp3 = nn.Linear(time_emb_dim, features[2])\n",
    "        self.time_mlp4 = nn.Linear(time_emb_dim, features[3])\n",
    "        # Encoder\n",
    "        self.encoder1 = ResidualBlock(in_channels, features[0])\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = ResidualBlock(features[0], features[1])\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = ResidualBlock(features[1], features[2])\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Bottleneck avec attention\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ResidualBlock(features[2], features[3]),\n",
    "            SelfAttention(features[3])\n",
    "        )\n",
    "        # Décodeur\n",
    "        self.up3 = nn.ConvTranspose2d(features[3], features[2], kernel_size=2, stride=2)\n",
    "        self.decoder3 = ResidualBlock(features[3], features[2])\n",
    "        self.attention3 = SelfAttention(features[2])\n",
    "        self.up2 = nn.ConvTranspose2d(features[2], features[1], kernel_size=2, stride=2)\n",
    "        self.decoder2 = ResidualBlock(features[2], features[1])\n",
    "        self.attention2 = SelfAttention(features[1])\n",
    "        self.up1 = nn.ConvTranspose2d(features[1], features[0], kernel_size=2, stride=2)\n",
    "        self.decoder1 = ResidualBlock(features[1], features[0])\n",
    "        # Convolution finale\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        # Injecte le time embedding dans chaque étape via une projection linéaire et un broadcast spatial\n",
    "        B = x.size(0)\n",
    "        time1 = self.time_mlp1(t_emb).view(B, -1, 1, 1)\n",
    "        time2 = self.time_mlp2(t_emb).view(B, -1, 1, 1)\n",
    "        time3 = self.time_mlp3(t_emb).view(B, -1, 1, 1)\n",
    "        time4 = self.time_mlp4(t_emb).view(B, -1, 1, 1)\n",
    "        \n",
    "        enc1 = self.encoder1(x) + time1\n",
    "        enc2 = self.encoder2(self.pool1(enc1)) + time2\n",
    "        enc3 = self.encoder3(self.pool2(enc2)) + time3\n",
    "        bottleneck = self.bottleneck(self.pool3(enc3)) + time4\n",
    "        dec3 = self.up3(bottleneck)\n",
    "        if dec3.size() != enc3.size():\n",
    "            dec3 = F.interpolate(dec3, size=enc3.size()[2:], mode='bilinear', align_corners=False)\n",
    "        dec3 = torch.cat([dec3, enc3], dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec3 = self.attention3(dec3)\n",
    "        dec2 = self.up2(dec3)\n",
    "        if dec2.size() != enc2.size():\n",
    "            dec2 = F.interpolate(dec2, size=enc2.size()[2:], mode='bilinear', align_corners=False)\n",
    "        dec2 = torch.cat([dec2, enc2], dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec2 = self.attention2(dec2)\n",
    "        dec1 = self.up1(dec2)\n",
    "        if dec1.size() != enc1.size():\n",
    "            dec1 = F.interpolate(dec1, size=enc1.size()[2:], mode='bilinear', align_corners=False)\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        out = self.final_conv(dec1)\n",
    "        if out.shape[-2:] != x.shape[-2:]:\n",
    "            out = F.interpolate(out, size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        return out\n",
    "\n",
    "###############################\n",
    "# 6) CONDITIONAL DIFFUSION MODEL (ACDM) WITH TIME EMBEDDING\n",
    "###############################\n",
    "class DiffusionModelACDM(nn.Module):\n",
    "    def init(self, diffusion_steps, cond_channels, data_channels, time_emb_dim=256, cond_noise_std=0.05):\n",
    "        \"\"\"\n",
    "        cond_noise_std: niveau d'écart-type du bruit ajouté au conditionnement pendant l'entraînement\n",
    "        \"\"\"\n",
    "        super(DiffusionModelACDM, self).init()\n",
    "        self.timesteps = diffusion_steps\n",
    "        self.cond_noise_std = cond_noise_std\n",
    "        # Le UNet prend la concaténation des données conditionnelles et des données cibles\n",
    "        self.unet = ImprovedDeepUNet(in_channels=cond_channels + data_channels, \n",
    "                                     out_channels=data_channels,\n",
    "                                     time_emb_dim=time_emb_dim)\n",
    "        self.time_embedding = TimeEmbedding(embedding_dim=time_emb_dim)\n",
    "        betas = self.linear_beta_schedule(diffusion_steps)  # (T,)\n",
    "        betas = betas.unsqueeze(1).unsqueeze(2).unsqueeze(3)  # (T,1,1,1)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        self.register_buffer(\"betas\", betas)\n",
    "        self.register_buffer(\"alphas_cumprod\", alphas_cumprod)\n",
    "        self.register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1.0 - alphas_cumprod))\n",
    "\n",
    "    def linear_beta_schedule(self, timesteps):\n",
    "        beta_start = 0.0001\n",
    "        beta_end = 0.02\n",
    "        return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "    def forward(self, cond_frames, cond_u_past, cond_u_curr, target_frame):\n",
    "        # cond_frames: (B, past_window, 1, H, W)\n",
    "        # cond_u_past: (B, past_window, 1)\n",
    "        # cond_u_curr: (B, 1)\n",
    "        # target_frame: (B, 1, H, W)\n",
    "        B, past_window, _, H, W = cond_frames.shape\n",
    "        # Reformater le conditionnement\n",
    "        cond_frames_map = cond_frames.view(B, past_window, H, W)\n",
    "        # AJOUT DE BRUIT AU CONDITIONNEMENT (pendant l'entraînement uniquement)\n",
    "        if self.training:\n",
    "            cond_frames_map = cond_frames_map + torch.randn_like(cond_frames_map) * self.cond_noise_std\n",
    "        u_past_map = cond_u_past.view(B, past_window, 1, 1).expand(B, past_window, H, W)\n",
    "        u_curr_map = cond_u_curr.view(B, 1, 1, 1).expand(B, 1, H, W)\n",
    "        cond = torch.cat([cond_frames_map, u_past_map, u_curr_map], dim=1)\n",
    "        \n",
    "        x0 = target_frame\n",
    "        t = torch.randint(0, self.timesteps, (B,), device=x0.device).long()\n",
    "        t_emb = self.time_embedding(t)\n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t].view(B, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t].view(B, 1, 1, 1)\n",
    "        noise = torch.randn_like(x0)\n",
    "        x_t = sqrt_alpha * x0 + sqrt_one_minus_alpha * noise\n",
    "        input_unet = torch.cat([cond, x_t], dim=1)\n",
    "        predicted_noise = self.unet(input_unet, t_emb)\n",
    "        return noise, predicted_noise\n",
    "\n",
    "    def sample(self, cond_frames, cond_u_past, cond_u_curr, num_steps=None):\n",
    "        # Lors de l'inférence, aucun bruit n'est ajouté au conditionnement\n",
    "        if num_steps is None:\n",
    "            num_steps = self.timesteps\n",
    "        B, past_window, _, H, W = cond_frames.shape\n",
    "        cond_frames_map = cond_frames.view(B, past_window, H, W)\n",
    "        u_past_map = cond_u_past.view(B, past_window, 1, 1).expand(B, past_window, H, W)\n",
    "        u_curr_map = cond_u_curr.view(B, 1, 1, 1).expand(B, 1, H, W)\n",
    "        cond = torch.cat([cond_frames_map, u_past_map, u_curr_map], dim=1)\n",
    "        x_t = torch.randn(B, 1, H, W, device=cond_frames.device)\n",
    "        for i in reversed(range(num_steps)):\n",
    "            t = torch.full((B,), i, device=cond_frames.device, dtype=torch.long)\n",
    "            t_emb = self.time_embedding(t)\n",
    "            sqrt_alpha = self.sqrt_alphas_cumprod[t].view(B, 1, 1, 1)\n",
    "            sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t].view(B, 1, 1, 1)\n",
    "            input_unet = torch.cat([cond, x_t], dim=1)\n",
    "            predicted_noise = self.unet(input_unet, t_emb)\n",
    "            x0_pred = (x_t - sqrt_one_minus_alpha * predicted_noise) / sqrt_alpha\n",
    "            if i > 0:\n",
    "                beta_t = self.betas[t].view(B, 1, 1, 1)\n",
    "                noise = torch.randn_like(x_t)\n",
    "                x_t = x0_pred * (1 - beta_t) + noise * beta_t\n",
    "            else:\n",
    "                x_t = x0_pred\n",
    "        return x_t\n",
    "\n",
    "###############################\n",
    "# 7) TRAINING LOOP WITH COSINE ANNEALING SCHEDULER AND TQDM\n",
    "###############################\n",
    "def train_diffusion_model(model, train_loader, val_loader, num_epochs, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for cond_frames, cond_u_past, cond_u_curr, target_frame in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} training\"):\n",
    "            cond_frames = cond_frames.to(device)\n",
    "            cond_u_past = cond_u_past.to(device)\n",
    "            cond_u_curr = cond_u_curr.to(device)\n",
    "            target_frame = target_frame.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            noise, pred_noise = model(cond_frames, cond_u_past, cond_u_curr, target_frame)\n",
    "            loss = F.smooth_l1_loss(pred_noise, noise)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for cond_frames, cond_u_past, cond_u_curr, target_frame in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} validation\"):\n",
    "                cond_frames = cond_frames.to(device)\n",
    "                cond_u_past = cond_u_past.to(device)\n",
    "                cond_u_curr = cond_u_curr.to(device)\n",
    "                target_frame = target_frame.to(device)\n",
    "                noise, pred_noise = model(cond_frames, cond_u_past, cond_u_curr, target_frame)\n",
    "                loss = F.smooth_l1_loss(pred_noise, noise)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.7f}, Val Loss: {val_loss:.7f}, LR: {scheduler.get_last_lr()[0]:.7f}\")\n",
    "        if val_loss < best_val_loss: \n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_diffusion_model.pth')\n",
    "            print(f\"New best model saved with validation loss: {best_val_loss:.7f}\")\n",
    "    print(\"Training complete!\") \n",
    "\n",
    "###############################\n",
    "# 8) EVALUATION ET INFÉRENCE AUTO-RÉGRESSIVE SANS RE-BRUITAGE DU CONDITIONNEMENT\n",
    "###############################\n",
    "def evaluate_single_test_autoregressive(model, data_case, past_window, device, num_frames, frame_mean, frame_std):\n",
    "    \"\"\"\n",
    "    Pour chaque frame prédite, on utilise le résultat tel quel pour mettre à jour l'historique,\n",
    "    car le conditionnement n'est pas bruité lors de l'inférence.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    frames = data_case['frames']  # frames normalisées\n",
    "    u = data_case['u']            # signal u normalisé\n",
    "    T = frames.shape[0]\n",
    "    history_frames = frames[:past_window].copy()  # (past_window, 1, H, W)\n",
    "    pred_frames = []\n",
    "    mse_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_frames):\n",
    "            t_idx = past_window + i\n",
    "            if t_idx >= T:\n",
    "                break\n",
    "            # Construction des conditions à partir de l'historique\n",
    "            cond_frames = history_frames[-past_window:]  # (past_window, 1, H, W)\n",
    "            cond_u_past = u[t_idx - past_window:t_idx].reshape(-1, 1)\n",
    "            cond_u_curr = np.array([u[t_idx]], dtype=np.float32)\n",
    "            cond_frames_tensor = torch.tensor(cond_frames, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            past_u_tensor = torch.tensor(cond_u_past, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            current_u_tensor = torch.tensor(cond_u_curr, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            \n",
    "            # Génération de la frame prédite\n",
    "            pred_frame = model.sample(cond_frames_tensor, past_u_tensor, current_u_tensor)\n",
    "            pred_frame_np = pred_frame.squeeze(0).cpu().numpy()  # (1, H, W)\n",
    "            \n",
    "            # Calcul de l'erreur (MSE) par rapport à la frame vraie\n",
    "            true_frame = frames[t_idx]  # (1, H, W)\n",
    "            mse = F.mse_loss(pred_frame.squeeze(0), torch.tensor(true_frame, dtype=torch.float32, device=device)).item()\n",
    "            mse_list.append(mse)\n",
    "            pred_frames.append(pred_frame.squeeze(0).cpu().numpy())\n",
    "            \n",
    "            # Mise à jour de l'historique sans ajout de bruit\n",
    "            history_frames = np.concatenate([history_frames, pred_frame_np[np.newaxis, ...]], axis=0)\n",
    "            \n",
    "    pred_frames = np.array(pred_frames)  # (num_frames, 1, H, W)\n",
    "    true_frames = frames[past_window:past_window+num_frames]  # (num_frames, 1, H, W)\n",
    "    true_frames_denorm = true_frames * frame_std + frame_mean\n",
    "    pred_frames_denorm = pred_frames * frame_std + frame_mean\n",
    "    avg_mse = sum(mse_list) / len(mse_list)\n",
    "    print(f\"Average MSE for amplitude {data_case['amplitude']} (autoregressive): {avg_mse:.6f}\")\n",
    "    return pred_frames_denorm, true_frames_denorm, avg_mse\n",
    "\n",
    "###############################\n",
    "# 9) CREATE COMPARISON VIDEO\n",
    "###############################\n",
    "def create_comparison_video(gt_frames, pred_frames, amplitude, save_path=\"comparison_video.mp4\"):\n",
    "    T = gt_frames.shape[0]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    fig.suptitle(f\"Amplitude: {amplitude}\")\n",
    "    ax1, ax2 = axes\n",
    "    ax1.set_title(\"Ground Truth\")\n",
    "    ax2.set_title(\"Predicted\")\n",
    "    vmin = min(gt_frames.min(), pred_frames.min())\n",
    "    vmax = max(gt_frames.max(), pred_frames.max())\n",
    "    sns.heatmap(gt_frames[0], cmap=\"magma\", vmin=vmin, vmax=vmax, center=0, ax=ax1, cbar=False, square=True)\n",
    "    sns.heatmap(pred_frames[0], cmap=\"magma\", vmin=vmin, vmax=vmax, center=0, ax=ax2, cbar=False, square=True)\n",
    "    def update(frame):\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "        sns.heatmap(gt_frames[frame], cmap=\"magma\", vmin=vmin, vmax=vmax, center=0, ax=ax1, cbar=False, square=True)\n",
    "        sns.heatmap(pred_frames[frame], cmap=\"magma\", vmin=vmin, vmax=vmax, center=0, ax=ax2, cbar=False, square=True)\n",
    "        ax1.set_title(f\"Ground Truth (Frame {frame+1}/{T})\")\n",
    "        ax2.set_title(f\"Predicted (Frame {frame+1}/{T})\")\n",
    "    ani = animation.FuncAnimation(fig, update, frames=T, interval=200)\n",
    "    ani.save(save_path, writer=\"ffmpeg\", fps=5, dpi=200)\n",
    "    print(f\"Vidéo sauvegardée sous : {save_path}\")\n",
    "    return save_path\n",
    "\n",
    "###############################\n",
    "# 10) MAIN SCRIPT\n",
    "###############################\n",
    "if name == \"main\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    file_path = \"oscillating_cylinder_benchmark_dataset_v2.mat\"\n",
    "    train_list, val_list, test_list = split_data(file_path)\n",
    "    print(f\"Training: {len(train_list)} amplitudes, Validation: {len(val_list)} amplitudes, Test: {len(test_list)} amplitudes\")\n",
    "    \n",
    "    frame_mean, frame_std, u_mean, u_std = compute_normalization_stats(train_list)\n",
    "    print(f\"Frame mean: {frame_mean:.6f}, Frame std: {frame_std:.6f}\")\n",
    "    print(f\"u mean: {u_mean:.6f}, u std: {u_std:.6f}\")\n",
    "    \n",
    "    normalize_data_list(train_list, frame_mean, frame_std, u_mean, u_std)\n",
    "    normalize_data_list(val_list, frame_mean, frame_std, u_mean, u_std)\n",
    "    normalize_data_list(test_list, frame_mean, frame_std, u_mean, u_std)\n",
    "    \n",
    "    past_window = 10\n",
    "    X_train_frames, X_train_u_past, X_train_u_curr, Y_train = create_acdm_sequences(train_list, past_window)\n",
    "    X_val_frames, X_val_u_past, X_val_u_curr, Y_val = create_acdm_sequences(val_list, past_window)\n",
    "    \n",
    "    train_dataset = ACylinderDataset(X_train_frames, X_train_u_past, X_train_u_curr, Y_train)\n",
    "    val_dataset = ACylinderDataset(X_val_frames, X_val_u_past, X_val_u_curr, Y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Le conditionnement est composé de past frames (multiplié par 2 pour y inclure u_past) et du u courant\n",
    "    cond_channels = 2 * past_window + 1  # past frames + past u + current u\n",
    "    data_channels = 1  # frame cible\n",
    "    diffusion_steps = 40\n",
    "    model = DiffusionModelACDM(diffusion_steps, cond_channels, data_channels, cond_noise_std=0.2).to(device)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    print(\"Début de l'entraînement avec bruitage du conditionnement pendant le training et scheduler cosine annealing...\")\n",
    "    train_diffusion_model(model, train_loader, val_loader, num_epochs, device)\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_diffusion_model.pth'))\n",
    "    \n",
    "    total_mse = 0\n",
    "    for i, test_case in enumerate(test_list):\n",
    "        print(f\"\\nEvaluation test case {i+1}/{len(test_list)} (amplitude: {test_case['amplitude']}) avec inférence auto-régressive...\")\n",
    "        pred_frames, true_frames, mse = evaluate_single_test_autoregressive(\n",
    "            model, test_case, past_window, device, num_frames=10, frame_mean=frame_mean, frame_std=frame_std\n",
    "        )\n",
    "        total_mse += mse\n",
    "        gt_sequence = true_frames.squeeze(1)\n",
    "        pred_sequence = pred_frames.squeeze(1)\n",
    "        video_path = f\"comparison_{test_case['amplitude']}_autoregressive.mp4\"\n",
    "        create_comparison_video(gt_sequence, pred_sequence, test_case['amplitude'], save_path=video_path)\n",
    "    \n",
    "    avg_test_mse = total_mse / len(test_list)\n",
    "    print(f\"\\nAverage MSE across all test cases (autoregressive): {avg_test_mse:.6f}\")\n",
    "    print(\"All done!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
