{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3375eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "###############################\n",
    "# 1) DATA LOADING & RESIZING\n",
    "###############################\n",
    "def load_swept_sine_case(file_path, amplitude_key, target_x=256, target_y=256):\n",
    "    \"\"\"RedimensionnÃ© en 256x256 pour Ãªtre divisible par 16 (patch size)\"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        dataset = f['data_structure']['swept_sines'][amplitude_key]\n",
    "        wz_grid = np.array(dataset['wz_grid'])  # (6405, T)\n",
    "        T = wz_grid.shape[1]\n",
    "        spatial_x, spatial_y = 61, 105\n",
    "        wz_3d = wz_grid.reshape(spatial_x, spatial_y, T)\n",
    "        u = np.array(dataset['y'])\n",
    "        if u.ndim > 1:\n",
    "            u = u.squeeze()\n",
    "        frames_resized = np.zeros((target_x, target_y, T), dtype=np.float32)\n",
    "        for t in range(T):\n",
    "            frames_resized[:, :, t] = scipy.ndimage.zoom(\n",
    "                wz_3d[:, :, t],\n",
    "                (target_x / spatial_x, target_y / spatial_y),\n",
    "                order=1\n",
    "            )\n",
    "        frames_resized = np.transpose(frames_resized, (2, 0, 1))  # (T, target_x, target_y)\n",
    "        frames_resized = frames_resized[:, np.newaxis, :, :]      # (T, 1, target_x, target_y)\n",
    "        return {\n",
    "            'amplitude': amplitude_key,\n",
    "            'u': u.astype(np.float32),\n",
    "            'frames': frames_resized\n",
    "        }\n",
    "\n",
    "def load_all_amplitudes(file_path, amplitude_map, amplitude_list):\n",
    "    data_list = []\n",
    "    for amp in amplitude_list:\n",
    "        amp_key = amplitude_map[amp]\n",
    "        data_case = load_swept_sine_case(file_path, amp_key, target_x=48, target_y=48)\n",
    "        data_list.append(data_case)\n",
    "    return data_list\n",
    "\n",
    "def split_data(file_path):\n",
    "    amplitude_map = {\n",
    "        0.5:  'A0p05',\n",
    "        0.75: 'A0p075',\n",
    "        1.0:  'A0p10',\n",
    "        1.25: 'A0p125',\n",
    "        1.5:  'A0p15',\n",
    "        1.75: 'A0p175',\n",
    "        2.0:  'A0p20',\n",
    "        2.25: 'A0p225',\n",
    "        2.5:  'A0p25',\n",
    "        2.75: 'A0p275',\n",
    "        3.0:  'A0p30'\n",
    "    }\n",
    "    train_amps = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "    val_amps   = [0.75, 1.75, 2.75]\n",
    "    test_amps  = [1.25, 2.25]\n",
    "    train_list = load_all_amplitudes(file_path, amplitude_map, train_amps)\n",
    "    val_list   = load_all_amplitudes(file_path, amplitude_map, val_amps)\n",
    "    test_list  = load_all_amplitudes(file_path, amplitude_map, test_amps)\n",
    "    return train_list, val_list, test_list\n",
    "\n",
    "###############################\n",
    "# 1.1) NORMALISATION DES DONNÃ‰ES\n",
    "###############################\n",
    "def compute_normalization_stats(data_list):\n",
    "    all_frames = np.concatenate([data['frames'] for data in data_list], axis=0)\n",
    "    frame_mean = all_frames.mean()\n",
    "    frame_std = all_frames.std()\n",
    "    all_u = np.concatenate([data['u'] for data in data_list], axis=0)\n",
    "    u_mean = all_u.mean()\n",
    "    u_std = all_u.std()\n",
    "    return frame_mean, frame_std, u_mean, u_std\n",
    "\n",
    "def normalize_data_list(data_list, frame_mean, frame_std, u_mean, u_std):\n",
    "    for data in data_list:\n",
    "        data['frames'] = (data['frames'] - frame_mean) / frame_std\n",
    "        data['u'] = (data['u'] - u_mean) / u_std\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "# 2) CREATING SEQUENCES FOR JiT\n",
    "###############################\n",
    "def create_jit_sequences(data_list, past_window=2, delta_t=0.1):\n",
    "    \"\"\"CrÃ©er des sÃ©quences pour l'entraÃ®nement avec JiT\"\"\"\n",
    "    X_frames_list, X_u_past_list, X_u_curr_list, Y_list = [], [], [], []\n",
    "    for data_case in data_list:\n",
    "        frames = data_case['frames']  # (T, 1, H, W)\n",
    "        u = data_case['u']            # (T,)\n",
    "        T = frames.shape[0]\n",
    "        for i in range(past_window, T):\n",
    "            past_f = frames[i-past_window:i]               # (past_window, 1, H, W)\n",
    "            past_u = u[i-past_window:i].reshape(-1, 1)     # (past_window, 1)\n",
    "            current_u = np.array([u[i]], dtype=np.float32) # (1,)\n",
    "            \n",
    "            target_f = frames[i]                           # (1, H, W)\n",
    "            \n",
    "\n",
    "            \n",
    "            X_frames_list.append(past_f)\n",
    "            X_u_past_list.append(past_u)\n",
    "            X_u_curr_list.append(current_u)\n",
    "            Y_list.append(target_f)\n",
    "            \n",
    "    X_frames = np.array(X_frames_list, dtype=np.float32)\n",
    "    X_u_past = np.array(X_u_past_list, dtype=np.float32)\n",
    "    X_u_curr = np.array(X_u_curr_list, dtype=np.float32)\n",
    "    Y = np.array(Y_list, dtype=np.float32)\n",
    "    return X_frames, X_u_past, X_u_curr, Y\n",
    "\n",
    "class FluidDynamicsDataset(Dataset):\n",
    "    def __init__(self, X_frames, X_u_past, X_u_curr, Y):\n",
    "        self.X_frames = X_frames\n",
    "        self.X_u_past = X_u_past\n",
    "        self.X_u_curr = X_u_curr\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_frames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_frames[idx], self.X_u_past[idx], self.X_u_curr[idx], self.Y[idx]\n",
    "    \n",
    "\n",
    "###############################\n",
    "# 3) JiT ARCHITECTURE COMPONENTS (from model_jit.py)\n",
    "###############################\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size):\n",
    "    \"\"\"Generate 2D sin-cos positional embedding\"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)\n",
    "    grid = np.stack(grid, axis=0)\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    return pos_embed\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)\n",
    "    return emb\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega\n",
    "    pos = pos.reshape(-1)\n",
    "    out = np.einsum('m,d->md', pos, omega)\n",
    "    emb_sin = np.sin(out)\n",
    "    emb_cos = np.cos(out)\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)\n",
    "    return emb\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return x / norm * self.weight\n",
    "\n",
    "def modulate(x, shift, scale):\n",
    "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
    "\n",
    "class BottleneckPatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=48, patch_size=16, in_chans=1, pca_dim=128, embed_dim=384, bias=True):\n",
    "        super().__init__()\n",
    "        self.img_size = (img_size, img_size)\n",
    "        self.patch_size = (patch_size, patch_size)\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj1 = nn.Conv2d(in_chans, pca_dim, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "        self.proj2 = nn.Conv2d(pca_dim, embed_dim, kernel_size=1, stride=1, bias=bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.proj2(self.proj1(x)).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class TimestepEmbedder(nn.Module):\n",
    "    def __init__(self, hidden_size, frequency_embedding_size=256):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, hidden_size, bias=True),\n",
    "        )\n",
    "        self.frequency_embedding_size = frequency_embedding_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def timestep_embedding(t, dim, max_period=10000):\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32, device=t.device) / half\n",
    "        )\n",
    "        args = t[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, t):\n",
    "        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n",
    "        t_emb = self.mlp(t_freq)\n",
    "        return t_emb\n",
    "\n",
    "class ControlEmbedder(nn.Module):\n",
    "    \"\"\"Embed control signals (u_past and u_curr) into hidden dimension\"\"\"\n",
    "    def __init__(self, past_window, hidden_size):\n",
    "        super().__init__()\n",
    "        # Embed past_window + 1 (past + current) control values\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(past_window + 1, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "        )\n",
    "    \n",
    "    def forward(self, u_past, u_curr):\n",
    "        # u_past: (B, past_window, 1), u_curr: (B, 1)\n",
    "        u_combined = torch.cat([u_past.squeeze(-1), u_curr], dim=1)  # (B, past_window + 1)\n",
    "        return self.mlp(u_combined)\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, dropout_p=0.0):\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1))\n",
    "    attn_bias = torch.zeros(query.size(0), 1, L, S, dtype=query.dtype, device=query.device)\n",
    "    \n",
    "    with torch.cuda.amp.autocast(enabled=False):\n",
    "        attn_weight = query.float() @ key.float().transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    return attn_weight @ value\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=True, qk_norm=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        \n",
    "        self.q_norm = RMSNorm(head_dim) if qk_norm else nn.Identity()\n",
    "        self.k_norm = RMSNorm(head_dim) if qk_norm else nn.Identity()\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        q = self.q_norm(q)\n",
    "        k = self.k_norm(k)\n",
    "        \n",
    "        x = scaled_dot_product_attention(q, k, v, dropout_p=self.attn_drop.p if self.training else 0.)\n",
    "        \n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, drop=0.0, bias=True):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(hidden_dim * 2 / 3)\n",
    "        self.w12 = nn.Linear(dim, 2 * hidden_dim, bias=bias)\n",
    "        self.w3 = nn.Linear(hidden_dim, dim, bias=bias)\n",
    "        self.ffn_dropout = nn.Dropout(drop)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x12 = self.w12(x)\n",
    "        x1, x2 = x12.chunk(2, dim=-1)\n",
    "        hidden = F.silu(x1) * x2\n",
    "        return self.w3(self.ffn_dropout(hidden))\n",
    "\n",
    "class FinalLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, patch_size, out_channels):\n",
    "        super().__init__()\n",
    "        self.norm_final = RMSNorm(hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True)\n",
    "        self.adaLN_modulation = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, 2 * hidden_size, bias=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, c):\n",
    "        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
    "        x = modulate(self.norm_final(x), shift, scale)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class JiTBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(hidden_size, eps=1e-6)\n",
    "        self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=True, qk_norm=True,\n",
    "                              attn_drop=attn_drop, proj_drop=proj_drop)\n",
    "        self.norm2 = RMSNorm(hidden_size, eps=1e-6)\n",
    "        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
    "        self.mlp = SwiGLUFFN(hidden_size, mlp_hidden_dim, drop=proj_drop)\n",
    "        self.adaLN_modulation = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, 6 * hidden_size, bias=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, c):\n",
    "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=-1)\n",
    "        x = x + gate_msa.unsqueeze(1) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\n",
    "        x = x + gate_mlp.unsqueeze(1) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "# 4) JiT-BASED CONDITIONAL DIFFUSION MODEL\n",
    "###############################\n",
    "class JiTFluidDiffusion(nn.Module):\n",
    "    def __init__( \n",
    "        self,\n",
    "        img_size=256,\n",
    "        patch_size=16,\n",
    "        in_channels=1,\n",
    "        past_window=10,\n",
    "        hidden_size=384,\n",
    "        depth=4,\n",
    "        num_heads=6,\n",
    "        mlp_ratio=4.0,\n",
    "        bottleneck_dim=64,\n",
    "        diffusion_steps=50,\n",
    "        P_mean=-0.8,\n",
    "        P_std=0.8,\n",
    "        t_eps=0.05,\n",
    "        noise_scale=1.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = in_channels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.past_window = past_window\n",
    "        self.timesteps = diffusion_steps\n",
    "        self.P_mean = P_mean\n",
    "        self.P_std = P_std\n",
    "        self.t_eps = t_eps\n",
    "        self.noise_scale = noise_scale\n",
    "        \n",
    "        # Embedders\n",
    "        self.t_embedder = TimestepEmbedder(hidden_size)\n",
    "        self.control_embedder = ControlEmbedder(past_window, hidden_size)\n",
    "        \n",
    "        # Patch embedding pour la target frame (bruitÃ©e)\n",
    "        self.x_embedder = BottleneckPatchEmbed(\n",
    "            img_size, patch_size, in_channels, bottleneck_dim, hidden_size, bias=True\n",
    "        )\n",
    "        \n",
    "        # Patch embedding pour les past frames de conditionnement\n",
    "        # On va concatÃ©ner past_window frames en entrÃ©e\n",
    "        self.cond_embedder = BottleneckPatchEmbed(\n",
    "            img_size, patch_size, past_window, bottleneck_dim, hidden_size, bias=True\n",
    "        )\n",
    "        \n",
    "        # Positional embedding\n",
    "        num_patches = self.x_embedder.num_patches\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            JiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio, attn_drop=0.0, proj_drop=0.0)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Final prediction layer\n",
    "        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)\n",
    "        \n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        def _basic_init(module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "        self.apply(_basic_init)\n",
    "        \n",
    "        # Initialize pos_embed\n",
    "        pos_embed = get_2d_sincos_pos_embed(\n",
    "            self.pos_embed.shape[-1], \n",
    "            int(self.x_embedder.num_patches ** 0.5)\n",
    "        )\n",
    "        # (B, num_patches, hidden_size)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        \n",
    "        # Initialize patch embeddings\n",
    "        for embedder in [self.x_embedder, self.cond_embedder]:\n",
    "            w1 = embedder.proj1.weight.data\n",
    "            nn.init.xavier_uniform_(w1.view([w1.shape[0], -1]))\n",
    "            w2 = embedder.proj2.weight.data\n",
    "            nn.init.xavier_uniform_(w2.view([w2.shape[0], -1]))\n",
    "            nn.init.constant_(embedder.proj2.bias, 0)\n",
    "        \n",
    "        # Zero-out adaLN modulation layers\n",
    "        for block in self.blocks:\n",
    "            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\n",
    "            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)\n",
    "        \n",
    "        # Zero-out output layers\n",
    "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)\n",
    "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)\n",
    "        nn.init.constant_(self.final_layer.linear.weight, 0)\n",
    "        nn.init.constant_(self.final_layer.linear.bias, 0)\n",
    "    \n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"x: (N, num_patches, patch_size**2 * C) -> imgs: (N, C, H, W)\"\"\"\n",
    "        p = self.patch_size\n",
    "        c = self.out_channels\n",
    "        h = w = int(x.shape[1] ** 0.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], c, h * p, w * p))\n",
    "        return imgs\n",
    "    \n",
    "    def sample_t(self, n, device):\n",
    "        \"\"\"Sample timesteps from logit-normal distribution\"\"\"\n",
    "        z = torch.randn(n, device=device) * self.P_std + self.P_mean\n",
    "        return torch.sigmoid(z)\n",
    "    \n",
    "    def forward(self, cond_frames, cond_u_past, cond_u_curr, target_frame):\n",
    "        \"\"\"\n",
    "        Training forward pass with x-prediction and v-loss\n",
    "        cond_frames: (B, past_window, 1, H, W)\n",
    "        cond_u_past: (B, past_window, 1)\n",
    "        cond_u_curr: (B, 1)\n",
    "        target_frame: (B, 1, H, W)\n",
    "        \"\"\"\n",
    "        B = target_frame.size(0)\n",
    "        device = target_frame.device\n",
    "        \n",
    "        # Sample timestep\n",
    "        t = self.sample_t(B, device).view(-1, *([1] * (target_frame.ndim - 1)))\n",
    "        t_flat = t.flatten()\n",
    "        \n",
    "        # Add noise to target frame (forward diffusion)\n",
    "        e = torch.randn_like(target_frame) * self.noise_scale\n",
    "        z_t = t * target_frame + (1 - t) * e\n",
    "        \n",
    "        # Ground truth velocity\n",
    "        v = (target_frame - z_t) / (1 - t).clamp_min(self.t_eps)\n",
    "        \n",
    "        # Embed conditioning\n",
    "        # (B, past_window, 1, H, W) -> (B, past_window, H, W)\n",
    "        cond_frames_reshaped = cond_frames.squeeze(2)\n",
    "        # (B, past_window, H, W) -> (B, num_patches, hidden_size)\n",
    "        cond_tokens = self.cond_embedder(cond_frames_reshaped)\n",
    "        \n",
    "        # Embed noisy target\n",
    "        x_tokens = self.x_embedder(z_t)  # (B, num_patches, hidden_size)\n",
    "        \n",
    "        # Combine conditioning and noisy target tokens\n",
    "        # Simple approach: add them\n",
    "        tokens = x_tokens + cond_tokens + self.pos_embed\n",
    "        \n",
    "        # Time and control embedding\n",
    "        t_emb = self.t_embedder(t_flat)\n",
    "        control_emb = self.control_embedder(cond_u_past, cond_u_curr)\n",
    "        c = t_emb + control_emb\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            tokens = block(tokens, c)\n",
    "        \n",
    "        # Final layer predicts x directly\n",
    "        x_pred_tokens = self.final_layer(tokens, c)\n",
    "        x_pred = self.unpatchify(x_pred_tokens)\n",
    "        \n",
    "        # Convert x_pred to v_pred for loss\n",
    "        v_pred = (x_pred - z_t) / (1 - t).clamp_min(self.t_eps)\n",
    "        \n",
    "        # v-loss\n",
    "        loss = F.smooth_l1_loss(v_pred, v)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def forward(self, cond_frames, cond_u_past, cond_u_curr, target_frame, delta_t):\n",
    "        \"\"\"\n",
    "        Training forward pass with solver-in-the-loop (Heun)\n",
    "        cond_frames: (B, past_window, 1, H, W)\n",
    "        cond_u_past: (B, past_window, 1)\n",
    "        cond_u_curr: (B, 1)\n",
    "        target_frame: (B, 1, H, W)\n",
    "        \"\"\"\n",
    "        B = target_frame.size(0)\n",
    "        device = target_frame.device\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # Current state = last conditioning frame\n",
    "        # --------------------------------------------------\n",
    "        x_curr = cond_frames[:, -1]   # (B, 1, H, W)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # Conditioning tokens (UNCHANGED)\n",
    "        # --------------------------------------------------\n",
    "        cond_frames_reshaped = cond_frames.squeeze(2)\n",
    "        cond_tokens = self.cond_embedder(cond_frames_reshaped)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # Time + control embedding (keep, but t = 1)\n",
    "        # --------------------------------------------------\n",
    "        t = torch.ones(B, device=device)\n",
    "        t_emb = self.t_embedder(t)\n",
    "        control_emb = self.control_embedder(cond_u_past, cond_u_curr)\n",
    "        c = t_emb + control_emb\n",
    "\n",
    "        # ==================================================\n",
    "        # k1\n",
    "        # ==================================================\n",
    "        x_tokens = self.x_embedder(x_curr)\n",
    "        tokens = x_tokens + cond_tokens + self.pos_embed\n",
    "\n",
    "        for block in self.blocks:\n",
    "            tokens = block(tokens, c)\n",
    "\n",
    "        x_dot_1_tokens = self.final_layer(tokens, c)\n",
    "        x_dot_1 = self.unpatchify(x_dot_1_tokens)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # Heun intermediate state\n",
    "        # --------------------------------------------------\n",
    "        x_tilde = x_curr + delta_t * x_dot_1\n",
    "\n",
    "        cond_frames_tilde = torch.cat(\n",
    "            [cond_frames[:, 1:], x_tilde.unsqueeze(1)], dim=1\n",
    "        )\n",
    "        cond_frames_tilde_reshaped = cond_frames_tilde.squeeze(2)\n",
    "        cond_tokens_tilde = self.cond_embedder(cond_frames_tilde_reshaped)\n",
    "\n",
    "        # ==================================================\n",
    "        # k2\n",
    "        # ==================================================\n",
    "        x_tokens_2 = self.x_embedder(x_tilde)\n",
    "        tokens_2 = x_tokens_2 + cond_tokens_tilde + self.pos_embed\n",
    "\n",
    "        for block in self.blocks:\n",
    "            tokens_2 = block(tokens_2, c)\n",
    "\n",
    "        x_dot_2_tokens = self.final_layer(tokens_2, c)\n",
    "        x_dot_2 = self.unpatchify(x_dot_2_tokens)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # Heun integration\n",
    "        # --------------------------------------------------\n",
    "        x_hat = x_curr + 0.5 * delta_t * (x_dot_1 + x_dot_2)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # Loss on frame\n",
    "        # --------------------------------------------------\n",
    "        loss = F.mse_loss(x_hat, target_frame)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        labels_dropped = self.drop_labels(labels) if self.training else labels\n",
    "\n",
    "        t = self.sample_t(x.size(0), device=x.device).view(-1, *([1] * (x.ndim - 1)))\n",
    "        e = torch.randn_like(x) * self.noise_scale\n",
    "\n",
    "        z = t * x + (1 - t) * e\n",
    "        v = (x - z) / (1 - t).clamp_min(self.t_eps)\n",
    "\n",
    "        x_pred = self.net(z, t.flatten(), labels_dropped)\n",
    "        v_pred = (x_pred - z) / (1 - t).clamp_min(self.t_eps)\n",
    "\n",
    "        # l2 loss\n",
    "        loss = (v - v_pred) ** 2\n",
    "        loss = loss.mean(dim=(1, 2, 3)).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    @torch.no_grad() \n",
    "    def sample(self, cond_frames, cond_u_past, cond_u_curr, num_steps=50, method='heun'):\n",
    "        \"\"\"\n",
    "        Generate a frame using reverse diffusion\n",
    "        \"\"\"\n",
    "        B = cond_frames.size(0)\n",
    "        device = cond_frames.device\n",
    "        H, W = self.img_size, self.img_size\n",
    "        \n",
    "        # Start from noise\n",
    "        z = self.noise_scale * torch.randn(B, 1, H, W, device=device)\n",
    "        \n",
    "        # Timesteps\n",
    "        timesteps = torch.linspace(0.0, 1.0, num_steps + 1, device=device)\n",
    "        \n",
    "        # Embed conditioning (constant throughout sampling)\n",
    "        cond_frames_reshaped = cond_frames.squeeze(2)\n",
    "        cond_tokens = self.cond_embedder(cond_frames_reshaped)\n",
    "        cond_tokens = cond_tokens + self.pos_embed\n",
    "        \n",
    "        # Control embedding (constant)\n",
    "        control_emb = self.control_embedder(cond_u_past, cond_u_curr)\n",
    "        \n",
    "        for i in range(num_steps):\n",
    "            t_curr = timesteps[i]\n",
    "            t_next = timesteps[i + 1]\n",
    "            \n",
    "            if method == 'euler':\n",
    "                z = self._euler_step(z, t_curr, t_next, cond_tokens, control_emb)\n",
    "            elif method == 'heun':\n",
    "                z = self._heun_step(z, t_curr, t_next, cond_tokens, control_emb)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown method: {method}\")\n",
    "        return z\n",
    "    \n",
    "    def _forward_sample(self, z, t, cond_tokens, control_emb):\n",
    "        \"\"\"Single forward pass during sampling\"\"\"\n",
    "        B = z.size(0)\n",
    "        t_scalar = t.expand(B)\n",
    "        \n",
    "        # Embed noisy sample\n",
    "        x_tokens = self.x_embedder(z)\n",
    "        tokens = x_tokens + cond_tokens\n",
    "        \n",
    "        # Time embedding\n",
    "        t_emb = self.t_embedder(t_scalar)\n",
    "        c = t_emb + control_emb\n",
    "        \n",
    "        # Transformer\n",
    "        for block in self.blocks:\n",
    "            tokens = block(tokens, c)\n",
    "        \n",
    "        # Predict x\n",
    "        x_pred_tokens = self.final_layer(tokens, c)\n",
    "        x_pred = self.unpatchify(x_pred_tokens)\n",
    "        \n",
    "        # Convert to velocity\n",
    "        t_broadcast = t.view(-1, *([1] * (z.ndim - 1)))\n",
    "        v_pred = (x_pred - z) / (1.0 - t_broadcast).clamp_min(self.t_eps)\n",
    "        \n",
    "        return v_pred\n",
    "    \n",
    "    def _euler_step(self, z, t_curr, t_next, cond_tokens, control_emb):\n",
    "        v_pred = self._forward_sample(z, t_curr, cond_tokens, control_emb)\n",
    "        z_next = z + (t_next - t_curr) * v_pred\n",
    "        return z_next\n",
    "    \n",
    "    def _heun_step(self, z, t_curr, t_next, cond_tokens, control_emb):\n",
    "        # First Euler step\n",
    "        v_pred_curr = self._forward_sample(z, t_curr, cond_tokens, control_emb)\n",
    "        z_euler = z + (t_next - t_curr) * v_pred_curr\n",
    "        \n",
    "        # Second evaluation\n",
    "        v_pred_next = self._forward_sample(z_euler, t_next, cond_tokens, control_emb)\n",
    "        \n",
    "        # Heun average\n",
    "        v_pred = 0.5 * (v_pred_curr + v_pred_next)\n",
    "        z_next = z + (t_next - t_curr) * v_pred\n",
    "        return z_next\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_x_dot(self, cond_frames, cond_u_past, cond_u_curr):\n",
    "        B = cond_frames.size(0)\n",
    "        device = cond_frames.device\n",
    "\n",
    "        x_curr = cond_frames[:, -1]\n",
    "\n",
    "        cond_frames_reshaped = cond_frames.squeeze(2)\n",
    "        cond_tokens = self.cond_embedder(cond_frames_reshaped)\n",
    "\n",
    "        t = torch.ones(B, device=device)\n",
    "        t_emb = self.t_embedder(t)\n",
    "        control_emb = self.control_embedder(cond_u_past, cond_u_curr)\n",
    "        c = t_emb + control_emb\n",
    "\n",
    "        x_tokens = self.x_embedder(x_curr)\n",
    "        tokens = x_tokens + cond_tokens + self.pos_embed\n",
    "\n",
    "        for block in self.blocks:\n",
    "            tokens = block(tokens, c)\n",
    "\n",
    "        x_dot_tokens = self.final_layer(tokens, c)\n",
    "        x_dot = self.unpatchify(x_dot_tokens)\n",
    "\n",
    "        return x_dot\n",
    "\n",
    "    \n",
    "    \n",
    "###############################\n",
    "# 5) TRAINING LOOP\n",
    "###############################\n",
    "def train_jit_diffusion(model, train_loader, val_loader, num_epochs, device, learning_rate=1e-4, training_delta_t=0.1):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.0)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for cond_frames, cond_u_past, cond_u_curr, target_frame in tqdm(\n",
    "            train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"\n",
    "        ):\n",
    "            cond_frames = cond_frames.to(device)\n",
    "            cond_u_past = cond_u_past.to(device)\n",
    "            cond_u_curr = cond_u_curr.to(device)\n",
    "            target_frame = target_frame.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = model(cond_frames, cond_u_past, cond_u_curr, target_frame, training_delta_t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for cond_frames, cond_u_past, cond_u_curr, target_frame in tqdm(\n",
    "                val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"\n",
    "            ):\n",
    "                cond_frames = cond_frames.to(device)\n",
    "                cond_u_past = cond_u_past.to(device)\n",
    "                cond_u_curr = cond_u_curr.to(device)\n",
    "                target_frame = target_frame.to(device)\n",
    "                \n",
    "                loss = model(cond_frames, cond_u_past, cond_u_curr, target_frame)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | LR: {scheduler.get_last_lr()[0]:.7f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_jit_fluid_model.pth')\n",
    "            print(f\"âœ“ New best model saved! Val Loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    print(\"\\nâœ… Training complete!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "# 6) AUTOREGRESSIVE EVALUATION (Heun / RK2)\n",
    "###############################\n",
    "@torch.no_grad()\n",
    "def evaluate_autoregressive(\n",
    "    model,\n",
    "    data_case,\n",
    "    past_window,\n",
    "    device,\n",
    "    num_frames,\n",
    "    frame_mean,\n",
    "    frame_std,\n",
    "    delta_t=0.1\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    frames = data_case['frames']  # (T, 1, H, W)\n",
    "    u = data_case['u']\n",
    "    T = frames.shape[0]\n",
    "\n",
    "    # initial history\n",
    "    history_frames = frames[:past_window].copy()\n",
    "\n",
    "    pred_frames = []\n",
    "    mse_list = []\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        t_idx = past_window + i\n",
    "        if t_idx >= T:\n",
    "            break\n",
    "\n",
    "        # -----------------------------\n",
    "        # Conditioning\n",
    "        # -----------------------------\n",
    "        cond_frames = history_frames[-past_window:]\n",
    "        cond_u_past = u[t_idx - past_window:t_idx].reshape(-1, 1)\n",
    "        cond_u_curr = np.array([u[t_idx]], dtype=np.float32)\n",
    "\n",
    "        cond_frames_t = torch.tensor(\n",
    "            cond_frames, dtype=torch.float32, device=device\n",
    "        ).unsqueeze(0)\n",
    "        u_past_t = torch.tensor(\n",
    "            cond_u_past, dtype=torch.float32, device=device\n",
    "        ).unsqueeze(0)\n",
    "        u_curr_t = torch.tensor(\n",
    "            cond_u_curr, dtype=torch.float32, device=device\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Current state\n",
    "        # -----------------------------\n",
    "        x_curr = cond_frames_t[:, -1]  # (B=1, 1, H, W)\n",
    "\n",
    "        # =============================\n",
    "        # Heun (RK2)\n",
    "        # =============================\n",
    "\n",
    "        # ---- k1 ----\n",
    "        x_dot_1 = model.predict_x_dot(\n",
    "            cond_frames_t, u_past_t, u_curr_t\n",
    "        )\n",
    "\n",
    "        x_tilde = x_curr + delta_t * x_dot_1\n",
    "\n",
    "        # update conditioning for k2\n",
    "        cond_frames_tilde = torch.cat(\n",
    "            [cond_frames_t[:, 1:], x_tilde.unsqueeze(1)], dim=1\n",
    "        )\n",
    "\n",
    "        # ---- k2 ----\n",
    "        x_dot_2 = model.predict_x_dot(\n",
    "            cond_frames_tilde, u_past_t, u_curr_t\n",
    "        )\n",
    "\n",
    "        # ---- Heun update ----\n",
    "        x_next = x_curr + 0.5 * delta_t * (x_dot_1 + x_dot_2)\n",
    "\n",
    "        # -----------------------------\n",
    "        # To numpy\n",
    "        # -----------------------------\n",
    "        x_next_np = x_next.squeeze(0).cpu().numpy()\n",
    "\n",
    "        pred_frames.append(x_next_np)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Error\n",
    "        # -----------------------------\n",
    "        true_frame = frames[t_idx]\n",
    "        mse = np.mean((x_next_np - true_frame) ** 2)\n",
    "        mse_list.append(mse)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Update history\n",
    "        # -----------------------------\n",
    "        history_frames = np.concatenate(\n",
    "            [history_frames, x_next_np[np.newaxis]], axis=0\n",
    "        )\n",
    "        history_frames = history_frames[-past_window:]\n",
    "\n",
    "    # ---------------------------------\n",
    "    # Stack & denormalize\n",
    "    # ---------------------------------\n",
    "    pred_frames = np.array(pred_frames)\n",
    "    true_frames = frames[past_window:past_window + len(pred_frames)]\n",
    "\n",
    "    pred_frames_denorm = pred_frames * frame_std + frame_mean\n",
    "    true_frames_denorm = true_frames * frame_std + frame_mean\n",
    "\n",
    "    avg_mse = float(np.mean(mse_list))\n",
    "    print(f\"Average MSE for amplitude {data_case['amplitude']}: {avg_mse:.6f}\")\n",
    "\n",
    "    return pred_frames_denorm, true_frames_denorm, avg_mse\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "# 7) VISUALIZATION\n",
    "###############################\n",
    "def create_comparison_video(gt_frames, pred_frames, amplitude, save_path=\"comparison_video.mp4\"):\n",
    "    T = gt_frames.shape[0]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    fig.suptitle(f\"Amplitude: {amplitude}\")\n",
    "    ax1, ax2 = axes\n",
    "    ax1.set_title(\"Ground Truth\")\n",
    "    ax2.set_title(\"Predicted (JiT)\")\n",
    "    \n",
    "    vmin = min(gt_frames.min(), pred_frames.min())\n",
    "    vmax = max(gt_frames.max(), pred_frames.max())\n",
    "    \n",
    "    def update(frame):\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "        sns.heatmap(gt_frames[frame], cmap=\"magma\", vmin=vmin, vmax=vmax,\n",
    "                    center=0, ax=ax1, cbar=False, square=True)\n",
    "        sns.heatmap(pred_frames[frame], cmap=\"magma\", vmin=vmin, vmax=vmax,\n",
    "                    center=0, ax=ax2, cbar=False, square=True)\n",
    "        ax1.set_title(f\"Ground Truth (Frame {frame+1}/{T})\")\n",
    "        ax2.set_title(f\"Predicted JiT (Frame {frame+1}/{T})\")\n",
    "    \n",
    "    ani = animation.FuncAnimation(fig, update, frames=T, interval=200)\n",
    "    ani.save(save_path, writer=\"ffmpeg\", fps=5, dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(f\"Video saved: {save_path}\")\n",
    "    return save_path\n",
    "\n",
    "\n",
    "###############################\n",
    "# 8) MAIN\n",
    "###############################\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"ðŸš€ Using device: {device}\")\n",
    "\n",
    "    # =============================\n",
    "    # PATH\n",
    "    # =============================\n",
    "    file_path = \"/kaggle/input/oscillating-cylinder-benchmark-dataset-v2/oscillating_cylinder_benchmark_dataset_v2.mat\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Dataset not found at {file_path}\")\n",
    "\n",
    "    print(f\"âœ“ File found: {file_path}\")\n",
    "\n",
    "    # =============================\n",
    "    # LOAD DATA\n",
    "    # =============================\n",
    "    print(\"\\n=== Loading data ===\")\n",
    "    train_list, val_list, test_list = split_data(file_path)\n",
    "    print(f\"Train: {len(train_list)}, Val: {len(val_list)}, Test: {len(test_list)}\")\n",
    "\n",
    "    # =============================\n",
    "    # NORMALIZATION\n",
    "    # =============================\n",
    "    print(\"\\n=== Normalization ===\")\n",
    "    frame_mean, frame_std, u_mean, u_std = compute_normalization_stats(train_list)\n",
    "    print(f\"Frame: Î¼={frame_mean:.6f}, Ïƒ={frame_std:.6f}\")\n",
    "    print(f\"Control u: Î¼={u_mean:.6f}, Ïƒ={u_std:.6f}\")\n",
    "\n",
    "    normalize_data_list(train_list, frame_mean, frame_std, u_mean, u_std)\n",
    "    normalize_data_list(val_list, frame_mean, frame_std, u_mean, u_std)\n",
    "    normalize_data_list(test_list, frame_mean, frame_std, u_mean, u_std)\n",
    "\n",
    "    # =============================\n",
    "    # SEQUENCES\n",
    "    # =============================\n",
    "    print(\"\\n=== Creating sequences ===\")\n",
    "    past_window = 10\n",
    "    X_train_frames, X_train_u_past, X_train_u_curr, Y_train = create_jit_sequences(train_list, past_window)\n",
    "    X_val_frames, X_val_u_past, X_val_u_curr, Y_val = create_jit_sequences(val_list, past_window)\n",
    "\n",
    "    print(f\"Train sequences: {X_train_frames.shape[0]}\")\n",
    "    print(f\"Val sequences: {X_val_frames.shape[0]}\")\n",
    "\n",
    "    train_dataset = FluidDynamicsDataset(X_train_frames, X_train_u_past, X_train_u_curr, Y_train)\n",
    "    val_dataset = FluidDynamicsDataset(X_val_frames, X_val_u_past, X_val_u_curr, Y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # =============================\n",
    "    # MODEL\n",
    "    # =============================\n",
    "    print(\"\\n=== Creating JiT model ===\")\n",
    "    model = JiTFluidDiffusion(\n",
    "        img_size=256,\n",
    "        patch_size=16,\n",
    "        in_channels=1,\n",
    "        past_window=past_window,\n",
    "        hidden_size=384,\n",
    "        depth=12,\n",
    "        num_heads=6,\n",
    "        mlp_ratio=4.0,\n",
    "        bottleneck_dim=64\n",
    "    ).to(device)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "    # =============================\n",
    "    # TRAIN\n",
    "    # =============================\n",
    "    print(\"\\n=== Training ===\")\n",
    "    NUM_EPOCHS = 20\n",
    "    DELTA_T_TRAIN = 1.0\n",
    "\n",
    "    train_jit_diffusion(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        device=device,\n",
    "        learning_rate=2e-4\n",
    "    )\n",
    "\n",
    "    # =============================\n",
    "    # EVALUATION\n",
    "    # =============================\n",
    "    print(\"\\n=== Evaluation on test set ===\")\n",
    "    model.load_state_dict(torch.load(\"best_jit_fluid_model.pth\", map_location=device))\n",
    "\n",
    "    DELTA_T_EVAL = 1.0\n",
    "    total_mse = 0.0\n",
    "\n",
    "    for i, test_case in enumerate(test_list):\n",
    "        print(f\"\\nðŸ“Š Test case {i+1}/{len(test_list)} (amplitude: {test_case['amplitude']})\")\n",
    "\n",
    "        pred_frames, true_frames, mse = evaluate_autoregressive(\n",
    "            model=model,\n",
    "            data_case=test_case,\n",
    "            past_window=past_window,\n",
    "            device=device,\n",
    "            num_frames=20,\n",
    "            frame_mean=frame_mean,\n",
    "            frame_std=frame_std,\n",
    "            delta_t=DELTA_T_EVAL\n",
    "        )\n",
    "\n",
    "        total_mse += mse\n",
    "\n",
    "        gt_seq = true_frames.squeeze(1)\n",
    "        pred_seq = pred_frames.squeeze(1)\n",
    "\n",
    "        video_path = f\"jit_comparison_amp_{test_case['amplitude']}.mp4\"\n",
    "        create_comparison_video(gt_seq, pred_seq, test_case['amplitude'], video_path)\n",
    "\n",
    "    avg_test_mse = total_mse / len(test_list)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"âœ… Average test MSE (normalized): {avg_test_mse:.6f}\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
